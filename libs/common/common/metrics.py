from __future__ import annotations

import threading
from collections import deque
from typing import Deque

from prometheus_client import CollectorRegistry, Counter, Gauge, Histogram, generate_latest


class RollingQuantiles:
    def __init__(self, max_samples: int = 4096) -> None:
        self._samples: Deque[float] = deque(maxlen=max_samples)
        self._lock = threading.Lock()

    def observe(self, value: float) -> None:
        with self._lock:
            self._samples.append(value)

    def quantile(self, q: float) -> float:
        with self._lock:
            if not self._samples:
                return 0.0
            values = sorted(self._samples)
        if len(values) == 1:
            return values[0]
        index = int(round((len(values) - 1) * q))
        return values[index]


class GatewayMetrics:
    def __init__(self, namespace: str = "llm_gateway") -> None:
        self.registry = CollectorRegistry()
        self._latencies = RollingQuantiles()

        self.request_count = Counter(
            f"{namespace}_request_count",
            "Total requests handled by gateway",
            labelnames=("status",),
            registry=self.registry,
        )
        self.inflight_requests = Gauge(
            f"{namespace}_inflight_requests",
            "Number of in-flight requests",
            registry=self.registry,
        )
        self.batch_size_histogram = Histogram(
            f"{namespace}_batch_size",
            "Observed worker batch sizes",
            buckets=(1, 2, 4, 8, 16, 32, 64),
            registry=self.registry,
        )
        self.request_latency_seconds = Histogram(
            f"{namespace}_request_latency_seconds",
            "End-to-end request latency",
            buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5, 10),
            registry=self.registry,
        )
        self.request_latency_p50_seconds = Gauge(
            f"{namespace}_request_latency_p50_seconds",
            "Rolling p50 request latency",
            registry=self.registry,
        )
        self.request_latency_p95_seconds = Gauge(
            f"{namespace}_request_latency_p95_seconds",
            "Rolling p95 request latency",
            registry=self.registry,
        )
        self.request_latency_p99_seconds = Gauge(
            f"{namespace}_request_latency_p99_seconds",
            "Rolling p99 request latency",
            registry=self.registry,
        )
        self.tokens_generated_total = Counter(
            f"{namespace}_tokens_generated",
            "Total streamed tokens from worker",
            registry=self.registry,
        )
        self.errors_total = Counter(
            f"{namespace}_errors",
            "Total errors by code",
            labelnames=("code",),
            registry=self.registry,
        )

    def observe_latency(self, duration_seconds: float) -> None:
        self.request_latency_seconds.observe(duration_seconds)
        self._latencies.observe(duration_seconds)
        self.request_latency_p50_seconds.set(self._latencies.quantile(0.50))
        self.request_latency_p95_seconds.set(self._latencies.quantile(0.95))
        self.request_latency_p99_seconds.set(self._latencies.quantile(0.99))

    def render(self) -> bytes:
        return generate_latest(self.registry)


class WorkerMetrics:
    def __init__(self, namespace: str = "llm_worker") -> None:
        self.registry = CollectorRegistry()
        self.batch_size_histogram = Histogram(
            f"{namespace}_batch_size",
            "Worker batch sizes",
            buckets=(1, 2, 4, 8, 16, 32, 64),
            registry=self.registry,
        )
        self.tokens_generated_total = Counter(
            f"{namespace}_tokens_generated",
            "Total tokens generated by worker",
            registry=self.registry,
        )
        self.errors_total = Counter(
            f"{namespace}_errors",
            "Total worker errors",
            labelnames=("code",),
            registry=self.registry,
        )
